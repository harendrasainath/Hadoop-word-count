{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Installing Hadoop\n"
      ],
      "metadata": {
        "id": "NKMKOdPoHvgR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "metadata": {
        "id": "qgOCQ6c4T51Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing the hadoop latest version\n",
        "!wget https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XrDpho55H245",
        "outputId": "7cac3fb3-4c91-45f4-c5f5-321dfd174472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-05-07 21:10:35--  https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz\n",
            "Resolving downloads.apache.org (downloads.apache.org)... 135.181.214.104, 88.99.95.219, 2a01:4f8:10a:201a::2, ...\n",
            "Connecting to downloads.apache.org (downloads.apache.org)|135.181.214.104|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 638660563 (609M) [application/x-gzip]\n",
            "Saving to: ‘hadoop-3.3.2.tar.gz’\n",
            "\n",
            "hadoop-3.3.2.tar.gz 100%[===================>] 609.07M  18.0MB/s    in 35s     \n",
            "\n",
            "2022-05-07 21:11:11 (17.4 MB/s) - ‘hadoop-3.3.2.tar.gz’ saved [638660563/638660563]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#unzipping\n",
        "!tar -xzf hadoop-3.3.2.tar.gz"
      ],
      "metadata": {
        "id": "WCcyLmCRIqQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copying the hadoop file to user/local\n",
        "!mv  hadoop-3.3.2/ /usr/local"
      ],
      "metadata": {
        "id": "koG67ZiII3OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Setting up the Environment\n"
      ],
      "metadata": {
        "id": "bWiYWPy4SW37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To find the default Java path\n",
        "!readlink -f /usr/bin/java | sed \"s:bin/java::\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i15sEXtfJZOR",
        "outputId": "5c989db6-ba1a-4d3a-e667-110f2eb6c713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib/jvm/java-11-openjdk-amd64/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up the java path\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" \n",
        "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop-3.3.2/\""
      ],
      "metadata": {
        "id": "nbTvtjhoR0wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idTnamXBSCOy",
        "outputId": "86e52f77-5e3f-4d6c-91bf-e7afb2f6953b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add hadoop bin to path\n",
        "current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'\n",
        "new_path = current_path+':/usr/local/hadoop-3.3.2/bin/'\n",
        "os.environ[\"PATH\"] = new_path\n",
        "!echo $PATH"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtCTu2sBSKIZ",
        "outputId": "2b000384-077f-4054-f853-4e06bbbcbcb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/usr/local/hadoop-3.3.2/bin/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Testing Hadoop Installation\n",
        "\n"
      ],
      "metadata": {
        "id": "MaBQkPmZScpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riiJz29CUYuC",
        "outputId": "4ecde433-96e9-42fa-bd89-6d1166e16a03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usage: hadoop [OPTIONS] SUBCOMMAND [SUBCOMMAND OPTIONS]\n",
            " or    hadoop [OPTIONS] CLASSNAME [CLASSNAME OPTIONS]\n",
            "  where CLASSNAME is a user-provided Java class\n",
            "\n",
            "  OPTIONS is none or any of:\n",
            "\n",
            "buildpaths                       attempt to add class files from build tree\n",
            "--config dir                     Hadoop config directory\n",
            "--debug                          turn on shell script debug mode\n",
            "--help                           usage information\n",
            "hostnames list[,of,host,names]   hosts to use in slave mode\n",
            "hosts filename                   list of hosts to use in slave mode\n",
            "loglevel level                   set the log4j level for this command\n",
            "workers                          turn on worker mode\n",
            "\n",
            "  SUBCOMMAND is one of:\n",
            "\n",
            "\n",
            "    Admin Commands:\n",
            "\n",
            "daemonlog     get/set the log level for each daemon\n",
            "\n",
            "    Client Commands:\n",
            "\n",
            "archive       create a Hadoop archive\n",
            "checknative   check native Hadoop and compression libraries availability\n",
            "classpath     prints the class path needed to get the Hadoop jar and the\n",
            "              required libraries\n",
            "conftest      validate configuration XML files\n",
            "credential    interact with credential providers\n",
            "distch        distributed metadata changer\n",
            "distcp        copy file or directories recursively\n",
            "dtutil        operations related to delegation tokens\n",
            "envvars       display computed Hadoop environment variables\n",
            "fs            run a generic filesystem user client\n",
            "gridmix       submit a mix of synthetic job, modeling a profiled from\n",
            "              production load\n",
            "jar <jar>     run a jar file. NOTE: please use \"yarn jar\" to launch YARN\n",
            "              applications, not this command.\n",
            "jnipath       prints the java.library.path\n",
            "kdiag         Diagnose Kerberos Problems\n",
            "kerbname      show auth_to_local principal conversion\n",
            "key           manage keys via the KeyProvider\n",
            "rumenfolder   scale a rumen input trace\n",
            "rumentrace    convert logs into a rumen trace\n",
            "s3guard       manage metadata on S3\n",
            "trace         view and modify Hadoop tracing settings\n",
            "version       print the version\n",
            "\n",
            "    Daemon Commands:\n",
            "\n",
            "kms           run KMS, the Key Management Server\n",
            "registrydns   run the registry DNS server\n",
            "\n",
            "SUBCOMMAND may print help when invoked w/o parameters or with -h.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Word Count with Hadoop\n"
      ],
      "metadata": {
        "id": "JEIkUX0wU_KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating mapper and reducer files "
      ],
      "metadata": {
        "id": "96b6Pm0O5B6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mapper.py\n",
        "import sys\n",
        "\n",
        "# input comes from STDIN (standard input)\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    # split the line into words\n",
        "    words = line.split()\n",
        "    # increase counters\n",
        "    for word in words:\n",
        "        # write the results to STDOUT (standard output);\n",
        "        # what we output here will be the input for the\n",
        "        # Reduce step, i.e. the input for reducer.py\n",
        "        #\n",
        "        # tab-delimited; the trivial word count is 1\n",
        "        print (word, 1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R72o3cbfVCS4",
        "outputId": "6cb0ca1a-e1f2-4297-8846-ede9356bc882"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile reducer.py\n",
        "#!/usr/bin/env python\n",
        "\n",
        "from operator import itemgetter\n",
        "import sys\n",
        "\n",
        "current_word = None\n",
        "current_count = 0\n",
        "word = None\n",
        "\n",
        "# input comes from STDIN\n",
        "for line in sys.stdin:\n",
        "    # remove leading and trailing whitespace\n",
        "    line = line.strip()\n",
        "    \n",
        "\n",
        "    # parse the input we got from mapper.py\n",
        "    word, count = line.split(' ')\n",
        "    # convert count (currently a string) to int\n",
        "    count = int(count)\n",
        "\n",
        "    # this IF-switch only works because Hadoop sorts map output\n",
        "    # by key (here: word) before it is passed to the reducer\n",
        "    if current_word == word:\n",
        "        current_count += count\n",
        "    else:\n",
        "        if current_word:\n",
        "            # write result to STDOUT\n",
        "            print (current_word, current_count)\n",
        "        current_count = count\n",
        "        current_word = word\n",
        "\n",
        "# do not forget to output the last word if needed!\n",
        "if current_word == word:\n",
        "    print (current_word, current_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt3u823r6MF9",
        "outputId": "458dce4f-f96d-456b-960d-82d5ed97e0b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing reducer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod u+rwx /content/mapper.py\n",
        "!chmod u+rwx /content/reducer.py"
      ],
      "metadata": {
        "id": "oZjg4gKSAYRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q /word_count_hadoop.txt"
      ],
      "metadata": {
        "id": "sD8xklpyICML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /word_count_hadoop.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RKJE-xpaKwY6",
        "outputId": "5d9d70b1-6d8c-4a53-823e-400b396e241c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is harendra sai nath\r\n",
            "I have opted for the data science course at the praxis business school\r\n",
            "the curriculum of the course was very interesting\r\n",
            "It covers all the aspects of a data scientist role such as \r\n",
            "python, sql, machine learning, statistics, data visualisation and some other topics such as mathematics, marketing research and other programming languages as well.\r\n",
            "This course is so intensive and exhaustive in nature.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find / -name 'hadoop-streaming*.jar'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eui_wt0tIMq1",
        "outputId": "decfc299-fd77-4741-e051-1f9cec5c4c6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/hadoop-3.3.2/share/hadoop/tools/lib/hadoop-streaming-3.3.2.jar\n",
            "/usr/local/hadoop-3.3.2/share/hadoop/tools/sources/hadoop-streaming-3.3.2-test-sources.jar\n",
            "/usr/local/hadoop-3.3.2/share/hadoop/tools/sources/hadoop-streaming-3.3.2-sources.jar\n",
            "find: ‘/proc/26/task/26/net’: Invalid argument\n",
            "find: ‘/proc/26/net’: Invalid argument\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove output directories\n",
        "!rm -r wc_out\n",
        "!rm -r wc2_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4WIZGYqIly8",
        "outputId": "911e7002-200f-4272-be37-f7d867942a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'wc_out': No such file or directory\n",
            "rm: cannot remove 'wc2_out': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!hadoop jar /usr/local/hadoop-3.3.2/share/hadoop/tools/lib/hadoop-streaming-3.3.2.jar -input /word_count_hadoop.txt -output /content/wc_out  -mapper 'python mapper.py'  -reducer 'python reducer.py'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XXTGTRtIVKr",
        "outputId": "690bec4e-6a36-44aa-c4d1-8e8e3d0561e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-05-07 21:17:07,283 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n",
            "2022-05-07 21:17:07,413 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n",
            "2022-05-07 21:17:07,413 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n",
            "2022-05-07 21:17:07,433 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-05-07 21:17:07,671 INFO mapred.FileInputFormat: Total input files to process : 1\n",
            "2022-05-07 21:17:07,691 INFO mapreduce.JobSubmitter: number of splits:1\n",
            "2022-05-07 21:17:08,036 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1769816372_0001\n",
            "2022-05-07 21:17:08,036 INFO mapreduce.JobSubmitter: Executing with tokens: []\n",
            "2022-05-07 21:17:08,251 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
            "2022-05-07 21:17:08,252 INFO mapreduce.Job: Running job: job_local1769816372_0001\n",
            "2022-05-07 21:17:08,260 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
            "2022-05-07 21:17:08,262 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
            "2022-05-07 21:17:08,268 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-05-07 21:17:08,268 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-05-07 21:17:08,323 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
            "2022-05-07 21:17:08,327 INFO mapred.LocalJobRunner: Starting task: attempt_local1769816372_0001_m_000000_0\n",
            "2022-05-07 21:17:08,365 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-05-07 21:17:08,365 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-05-07 21:17:08,392 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-05-07 21:17:08,402 INFO mapred.MapTask: Processing split: file:/word_count_hadoop.txt:0+435\n",
            "2022-05-07 21:17:08,428 INFO mapred.MapTask: numReduceTasks: 1\n",
            "2022-05-07 21:17:08,616 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
            "2022-05-07 21:17:08,617 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
            "2022-05-07 21:17:08,617 INFO mapred.MapTask: soft limit at 83886080\n",
            "2022-05-07 21:17:08,617 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
            "2022-05-07 21:17:08,618 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
            "2022-05-07 21:17:08,622 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
            "2022-05-07 21:17:08,632 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, mapper.py]\n",
            "2022-05-07 21:17:08,638 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
            "2022-05-07 21:17:08,641 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
            "2022-05-07 21:17:08,642 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
            "2022-05-07 21:17:08,642 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
            "2022-05-07 21:17:08,643 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
            "2022-05-07 21:17:08,643 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
            "2022-05-07 21:17:08,645 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
            "2022-05-07 21:17:08,645 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
            "2022-05-07 21:17:08,646 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
            "2022-05-07 21:17:08,646 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
            "2022-05-07 21:17:08,647 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
            "2022-05-07 21:17:08,648 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
            "2022-05-07 21:17:08,689 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-05-07 21:17:08,787 INFO streaming.PipeMapRed: Records R/W=6/1\n",
            "2022-05-07 21:17:08,791 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-05-07 21:17:08,792 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-05-07 21:17:08,796 INFO mapred.LocalJobRunner: \n",
            "2022-05-07 21:17:08,796 INFO mapred.MapTask: Starting flush of map output\n",
            "2022-05-07 21:17:08,796 INFO mapred.MapTask: Spilling map output\n",
            "2022-05-07 21:17:08,796 INFO mapred.MapTask: bufstart = 0; bufend = 638; bufvoid = 104857600\n",
            "2022-05-07 21:17:08,796 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214120(104856480); length = 277/6553600\n",
            "2022-05-07 21:17:08,809 INFO mapred.MapTask: Finished spill 0\n",
            "2022-05-07 21:17:08,824 INFO mapred.Task: Task:attempt_local1769816372_0001_m_000000_0 is done. And is in the process of committing\n",
            "2022-05-07 21:17:08,832 INFO mapred.LocalJobRunner: Records R/W=6/1\n",
            "2022-05-07 21:17:08,832 INFO mapred.Task: Task 'attempt_local1769816372_0001_m_000000_0' done.\n",
            "2022-05-07 21:17:08,847 INFO mapred.Task: Final Counters for attempt_local1769816372_0001_m_000000_0: Counters: 17\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=141810\n",
            "\t\tFILE: Number of bytes written=782419\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6\n",
            "\t\tMap output records=70\n",
            "\t\tMap output bytes=638\n",
            "\t\tMap output materialized bytes=784\n",
            "\t\tInput split bytes=79\n",
            "\t\tCombine input records=0\n",
            "\t\tSpilled Records=70\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=0\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=435\n",
            "2022-05-07 21:17:08,847 INFO mapred.LocalJobRunner: Finishing task: attempt_local1769816372_0001_m_000000_0\n",
            "2022-05-07 21:17:08,851 INFO mapred.LocalJobRunner: map task executor complete.\n",
            "2022-05-07 21:17:08,859 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
            "2022-05-07 21:17:08,860 INFO mapred.LocalJobRunner: Starting task: attempt_local1769816372_0001_r_000000_0\n",
            "2022-05-07 21:17:08,870 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n",
            "2022-05-07 21:17:08,870 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
            "2022-05-07 21:17:08,871 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n",
            "2022-05-07 21:17:08,875 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3ec30ce1\n",
            "2022-05-07 21:17:08,877 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n",
            "2022-05-07 21:17:08,907 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2119434240, maxSingleShuffleLimit=529858560, mergeThreshold=1398826624, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
            "2022-05-07 21:17:08,921 INFO reduce.EventFetcher: attempt_local1769816372_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
            "2022-05-07 21:17:08,971 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local1769816372_0001_m_000000_0 decomp: 780 len: 784 to MEMORY\n",
            "2022-05-07 21:17:08,976 INFO reduce.InMemoryMapOutput: Read 780 bytes from map-output for attempt_local1769816372_0001_m_000000_0\n",
            "2022-05-07 21:17:08,978 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 780, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->780\n",
            "2022-05-07 21:17:08,981 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
            "2022-05-07 21:17:08,982 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-05-07 21:17:08,982 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
            "2022-05-07 21:17:08,991 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-05-07 21:17:08,991 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 774 bytes\n",
            "2022-05-07 21:17:08,995 INFO reduce.MergeManagerImpl: Merged 1 segments, 780 bytes to disk to satisfy reduce memory limit\n",
            "2022-05-07 21:17:09,001 INFO reduce.MergeManagerImpl: Merging 1 files, 784 bytes from disk\n",
            "2022-05-07 21:17:09,002 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
            "2022-05-07 21:17:09,002 INFO mapred.Merger: Merging 1 sorted segments\n",
            "2022-05-07 21:17:09,005 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 774 bytes\n",
            "2022-05-07 21:17:09,006 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-05-07 21:17:09,018 INFO streaming.PipeMapRed: PipeMapRed exec [/usr/local/bin/python, reducer.py]\n",
            "2022-05-07 21:17:09,021 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
            "2022-05-07 21:17:09,024 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
            "2022-05-07 21:17:09,056 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-05-07 21:17:09,056 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
            "2022-05-07 21:17:09,139 INFO streaming.PipeMapRed: Records R/W=70/1\n",
            "2022-05-07 21:17:09,143 INFO streaming.PipeMapRed: MRErrorThread done\n",
            "2022-05-07 21:17:09,144 INFO streaming.PipeMapRed: mapRedFinished\n",
            "2022-05-07 21:17:09,145 INFO mapred.Task: Task:attempt_local1769816372_0001_r_000000_0 is done. And is in the process of committing\n",
            "2022-05-07 21:17:09,146 INFO mapred.LocalJobRunner: 1 / 1 copied.\n",
            "2022-05-07 21:17:09,146 INFO mapred.Task: Task attempt_local1769816372_0001_r_000000_0 is allowed to commit now\n",
            "2022-05-07 21:17:09,148 INFO output.FileOutputCommitter: Saved output of task 'attempt_local1769816372_0001_r_000000_0' to file:/content/wc_out\n",
            "2022-05-07 21:17:09,157 INFO mapred.LocalJobRunner: Records R/W=70/1 > reduce\n",
            "2022-05-07 21:17:09,159 INFO mapred.Task: Task 'attempt_local1769816372_0001_r_000000_0' done.\n",
            "2022-05-07 21:17:09,160 INFO mapred.Task: Final Counters for attempt_local1769816372_0001_r_000000_0: Counters: 24\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=143410\n",
            "\t\tFILE: Number of bytes written=783738\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=54\n",
            "\t\tReduce shuffle bytes=784\n",
            "\t\tReduce input records=70\n",
            "\t\tReduce output records=54\n",
            "\t\tSpilled Records=70\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=258473984\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=535\n",
            "2022-05-07 21:17:09,160 INFO mapred.LocalJobRunner: Finishing task: attempt_local1769816372_0001_r_000000_0\n",
            "2022-05-07 21:17:09,160 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
            "2022-05-07 21:17:09,258 INFO mapreduce.Job: Job job_local1769816372_0001 running in uber mode : false\n",
            "2022-05-07 21:17:09,259 INFO mapreduce.Job:  map 100% reduce 100%\n",
            "2022-05-07 21:17:09,260 INFO mapreduce.Job: Job job_local1769816372_0001 completed successfully\n",
            "2022-05-07 21:17:09,272 INFO mapreduce.Job: Counters: 30\n",
            "\tFile System Counters\n",
            "\t\tFILE: Number of bytes read=285220\n",
            "\t\tFILE: Number of bytes written=1566157\n",
            "\t\tFILE: Number of read operations=0\n",
            "\t\tFILE: Number of large read operations=0\n",
            "\t\tFILE: Number of write operations=0\n",
            "\tMap-Reduce Framework\n",
            "\t\tMap input records=6\n",
            "\t\tMap output records=70\n",
            "\t\tMap output bytes=638\n",
            "\t\tMap output materialized bytes=784\n",
            "\t\tInput split bytes=79\n",
            "\t\tCombine input records=0\n",
            "\t\tCombine output records=0\n",
            "\t\tReduce input groups=54\n",
            "\t\tReduce shuffle bytes=784\n",
            "\t\tReduce input records=70\n",
            "\t\tReduce output records=54\n",
            "\t\tSpilled Records=140\n",
            "\t\tShuffled Maps =1\n",
            "\t\tFailed Shuffles=0\n",
            "\t\tMerged Map outputs=1\n",
            "\t\tGC time elapsed (ms)=0\n",
            "\t\tTotal committed heap usage (bytes)=516947968\n",
            "\tShuffle Errors\n",
            "\t\tBAD_ID=0\n",
            "\t\tCONNECTION=0\n",
            "\t\tIO_ERROR=0\n",
            "\t\tWRONG_LENGTH=0\n",
            "\t\tWRONG_MAP=0\n",
            "\t\tWRONG_REDUCE=0\n",
            "\tFile Input Format Counters \n",
            "\t\tBytes Read=435\n",
            "\tFile Output Format Counters \n",
            "\t\tBytes Written=535\n",
            "2022-05-07 21:17:09,274 INFO streaming.StreamJob: Output directory: /content/wc_out\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check output directory\n",
        "!ls wc_out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfistJhxJ_gE",
        "outputId": "7f7e5115-d585-4219-ccca-864944f722a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "part-00000  _SUCCESS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head wc_out/part-00000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4Nm1U6VKCg7",
        "outputId": "a1dacdb3-32a6-4dbf-e3c2-57b343d75fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I 1\t\n",
            "It 1\t\n",
            "My 1\t\n",
            "This 1\t\n",
            "a 1\t\n",
            "all 1\t\n",
            "and 3\t\n",
            "as 3\t\n",
            "aspects 1\t\n",
            "at 1\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sort -nr -k 2 -t$'\\t' wc_out/part-00000 > sorted.txt"
      ],
      "metadata": {
        "id": "D0JnnWHpKGQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /word_count_hadoop.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3Gqw7W3NX1Z",
        "outputId": "f35373cf-5030-46b8-8f11-160f85abf746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My name is harendra sai nath\r\n",
            "I have opted for the data science course at the praxis business school\r\n",
            "the curriculum of the course was very interesting\r\n",
            "It covers all the aspects of a data scientist role such as \r\n",
            "python, sql, machine learning, statistics, data visualisation and some other topics such as mathematics, marketing research and other programming languages as well.\r\n",
            "This course is so intensive and exhaustive in nature.\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -60 sorted.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XId8ceabNI95",
        "outputId": "56626e6b-af73-4fab-9582-43f2da62b1e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "well. 1\t\n",
            "was 1\t\n",
            "visualisation 1\t\n",
            "very 1\t\n",
            "topics 1\t\n",
            "This 1\t\n",
            "the 5\t\n",
            "such 2\t\n",
            "statistics, 1\t\n",
            "sql, 1\t\n",
            "some 1\t\n",
            "so 1\t\n",
            "scientist 1\t\n",
            "science 1\t\n",
            "school 1\t\n",
            "sai 1\t\n",
            "role 1\t\n",
            "research 1\t\n",
            "python, 1\t\n",
            "programming 1\t\n",
            "praxis 1\t\n",
            "other 2\t\n",
            "opted 1\t\n",
            "of 2\t\n",
            "nature. 1\t\n",
            "nath 1\t\n",
            "name 1\t\n",
            "My 1\t\n",
            "mathematics, 1\t\n",
            "marketing 1\t\n",
            "machine 1\t\n",
            "learning, 1\t\n",
            "languages 1\t\n",
            "It 1\t\n",
            "is 2\t\n",
            "interesting 1\t\n",
            "intensive 1\t\n",
            "in 1\t\n",
            "I 1\t\n",
            "have 1\t\n",
            "harendra 1\t\n",
            "for 1\t\n",
            "exhaustive 1\t\n",
            "data 3\t\n",
            "curriculum 1\t\n",
            "covers 1\t\n",
            "course 3\t\n",
            "business 1\t\n",
            "at 1\t\n",
            "aspects 1\t\n",
            "as 3\t\n",
            "and 3\t\n",
            "all 1\t\n",
            "a 1\t\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -30 sorted.txt\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmiT1TIkNO9z",
        "outputId": "33ce3d4d-f5ce-4058-853c-66d647477095"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nature. 1\t\n",
            "nath 1\t\n",
            "name 1\t\n",
            "My 1\t\n",
            "mathematics, 1\t\n",
            "marketing 1\t\n",
            "machine 1\t\n",
            "learning, 1\t\n",
            "languages 1\t\n",
            "It 1\t\n",
            "is 2\t\n",
            "interesting 1\t\n",
            "intensive 1\t\n",
            "in 1\t\n",
            "I 1\t\n",
            "have 1\t\n",
            "harendra 1\t\n",
            "for 1\t\n",
            "exhaustive 1\t\n",
            "data 3\t\n",
            "curriculum 1\t\n",
            "covers 1\t\n",
            "course 3\t\n",
            "business 1\t\n",
            "at 1\t\n",
            "aspects 1\t\n",
            "as 3\t\n",
            "and 3\t\n",
            "all 1\t\n",
            "a 1\t\n"
          ]
        }
      ]
    }
  ]
}